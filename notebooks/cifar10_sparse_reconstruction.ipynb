{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Sparse CIFAR-10 Reconstruction with DDO\n",
    "\n",
    "This notebook demonstrates sparse image reconstruction:\n",
    "- **Task**: Given 10% observed pixels (random locations), reconstruct the full image\n",
    "- **Training**: Use another 10% pixels as query/target for supervision\n",
    "- **Model**: Conditional DDO that takes observed pixels as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
    "display(HTML(\"<style>.output_result { max-width:98% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from utils import datasets\n",
    "from utils.sparse_datasets import SparseImageDatasetWrapper, create_sparse_mask_image\n",
    "from utils.visualize import get_grid_image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data-header",
   "metadata": {},
   "source": [
    "## 1. Load and Visualize Sparse CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-cifar10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base CIFAR-10 dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "base_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='../data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(f\"CIFAR-10 dataset loaded: {len(base_dataset)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-sparse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sparse dataset wrapper\n",
    "sparse_dataset = SparseImageDatasetWrapper(\n",
    "    dataset=base_dataset,\n",
    "    context_ratio=0.1,  # 10% observed pixels\n",
    "    query_ratio=0.1,    # 10% query pixels for training\n",
    "    mode='train',\n",
    "    return_full_image=True  # For visualization\n",
    ")\n",
    "\n",
    "print(sparse_dataset)\n",
    "print(f\"\\nContext points: {sparse_dataset.num_context} pixels\")\n",
    "print(f\"Query points: {sparse_dataset.num_query} pixels\")\n",
    "print(f\"Total pixels: {sparse_dataset.num_pixels} pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## 2. Visualize Sparse Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample\n",
    "sample = sparse_dataset[0]\n",
    "\n",
    "# Original image\n",
    "original_image = sample['image']\n",
    "\n",
    "# Create masked visualization (only showing observed pixels)\n",
    "masked_image = create_sparse_mask_image(\n",
    "    original_image,\n",
    "    sample['context_indices'],\n",
    "    fill_value=0.5  # Gray for unobserved\n",
    ")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].imshow(original_image.permute(1, 2, 0))\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(masked_image.permute(1, 2, 0))\n",
    "axes[1].set_title(f'Observed Pixels (10% = {sparse_dataset.num_context} pixels)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Context coords shape: {sample['context_coords'].shape}\")\n",
    "print(f\"Context values shape: {sample['context_values'].shape}\")\n",
    "print(f\"Query coords shape: {sample['query_coords'].shape}\")\n",
    "print(f\"Query values shape: {sample['query_values'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch-viz-header",
   "metadata": {},
   "source": [
    "## 3. Visualize Multiple Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a batch of sparse samples\n",
    "num_samples = 16\n",
    "nrow = 4\n",
    "\n",
    "originals = []\n",
    "masked = []\n",
    "\n",
    "for i in range(num_samples):\n",
    "    sample = sparse_dataset[i]\n",
    "    originals.append(sample['image'])\n",
    "    masked.append(create_sparse_mask_image(\n",
    "        sample['image'],\n",
    "        sample['context_indices'],\n",
    "        fill_value=0.5\n",
    "    ))\n",
    "\n",
    "originals = torch.stack(originals)\n",
    "masked = torch.stack(masked)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 8))\n",
    "\n",
    "# Original images\n",
    "grid_orig = get_grid_image(originals, nrow=nrow, pad_value=0, padding=2, to_numpy=True)\n",
    "axes[0].imshow(grid_orig)\n",
    "axes[0].set_title('Original Images')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Sparse observations\n",
    "grid_masked = get_grid_image(masked, nrow=nrow, pad_value=0, padding=2, to_numpy=True)\n",
    "axes[1].imshow(grid_masked)\n",
    "axes[1].set_title('Sparse Observations (10% pixels)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## 4. Training Setup (Demonstration)\n",
    "\n",
    "This shows how the data would be used in training. The actual training loop would:\n",
    "1. Take context points as conditional input\n",
    "2. Predict query point values\n",
    "3. Compute loss on query points only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader\n",
    "from utils.sparse_datasets import collate_sparse_batch\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    sparse_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_sparse_batch\n",
    ")\n",
    "\n",
    "# Get a batch\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(\"Batch contents:\")\n",
    "for key, val in batch.items():\n",
    "    if isinstance(val, torch.Tensor):\n",
    "        print(f\"  {key}: {val.shape}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(val)}\")\n",
    "\n",
    "print(\"\\nTraining workflow:\")\n",
    "print(\"  1. Input: context_coords + context_values (observed pixels)\")\n",
    "print(\"  2. Model predicts: values at query_coords\")\n",
    "print(\"  3. Loss: MSE between predicted and query_values\")\n",
    "print(\"  4. At inference: predict all pixels given context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mask-types-header",
   "metadata": {},
   "source": [
    "## 5. Different Mask Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mask-patterns",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sparse_datasets import GridMaskGenerator\n",
    "\n",
    "# Get a sample image\n",
    "sample_img, _ = base_dataset[0]\n",
    "C, H, W = sample_img.shape\n",
    "num_samples = int(H * W * 0.1)\n",
    "\n",
    "# Generate different mask patterns\n",
    "masks = {\n",
    "    'Random': GridMaskGenerator.random_mask(H * W, num_samples),\n",
    "    'Grid (stride=3)': GridMaskGenerator.grid_mask(H, W, stride=3),\n",
    "    'Center': GridMaskGenerator.center_mask(H, W, num_samples),\n",
    "}\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, len(masks) + 1, figsize=(15, 4))\n",
    "\n",
    "axes[0].imshow(sample_img.permute(1, 2, 0))\n",
    "axes[0].set_title('Original')\n",
    "axes[0].axis('off')\n",
    "\n",
    "for idx, (name, mask) in enumerate(masks.items()):\n",
    "    masked = create_sparse_mask_image(sample_img, mask, fill_value=0.5)\n",
    "    axes[idx + 1].imshow(masked.permute(1, 2, 0))\n",
    "    axes[idx + 1].set_title(f'{name}\\n({len(mask)} pixels)')\n",
    "    axes[idx + 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "notes-header",
   "metadata": {},
   "source": [
    "## Notes on Training\n",
    "\n",
    "To train a DDO model for sparse reconstruction:\n",
    "\n",
    "1. **Model Architecture**: Use conditional DDO that takes context points as input\n",
    "   - Context encoding: Process observed (coord, value) pairs\n",
    "   - Diffusion: Apply to full image representation\n",
    "   - Decoding: Generate values at all pixel locations\n",
    "\n",
    "2. **Training Loss**: Compute on query points only\n",
    "   ```python\n",
    "   # Pseudo-code\n",
    "   pred = model(context_coords, context_values, query_coords)\n",
    "   loss = mse_loss(pred, query_values)\n",
    "   ```\n",
    "\n",
    "3. **Inference**: Predict all unobserved pixels\n",
    "   ```python\n",
    "   # Get all pixel coordinates\n",
    "   all_coords = create_meshgrid(H, W)\n",
    "   reconstructed = model(context_coords, context_values, all_coords)\n",
    "   ```\n",
    "\n",
    "4. **Key Hyperparameters**:\n",
    "   - `context_ratio=0.1`: 10% observed pixels\n",
    "   - `query_ratio=0.1`: 10% query pixels for training\n",
    "   - Random sampling during training (different masks each iteration)\n",
    "   - Fixed or grid sampling for evaluation\n",
    "\n",
    "5. **Evaluation Metrics**:\n",
    "   - PSNR on reconstructed vs. ground truth\n",
    "   - SSIM for perceptual quality\n",
    "   - MSE on unobserved regions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
