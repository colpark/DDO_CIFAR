{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Sparse Image Reconstruction Training with DDO\n",
    "\n",
    "Complete training pipeline for conditional image reconstruction:\n",
    "- **Input**: 10% randomly observed pixels\n",
    "- **Output**: Full reconstructed 32Ã—32 RGB image\n",
    "- **Model**: Conditional DDO with sparse context encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
    "display(HTML(\"<style>.output_result { max-width:98% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Change to parent directory\n",
    "os.chdir('..')\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-utils",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import after main.py\n",
    "from utils.sparse_datasets import SparseImageDatasetWrapper, create_sparse_mask_image\n",
    "from utils.visualize import get_grid_image\n",
    "from utils.utils import Writer, count_parameters_in_M\n",
    "from main_sparse_reconstruction import (\n",
    "    init_model, get_mgrid, create_context_conditioning, get_args\n",
    ")\n",
    "\n",
    "# Re-import tqdm\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration\n",
    "import argparse\n",
    "\n",
    "args = argparse.Namespace()\n",
    "\n",
    "# Paths\n",
    "args.exp_path = './experiments/sparse_recon_notebook'\n",
    "args.data = './data'\n",
    "args.seed = 1\n",
    "args.command_type = 'train'\n",
    "\n",
    "# Dataset\n",
    "args.dataset = 'cifar10'\n",
    "args.train_img_height = 32\n",
    "args.input_dim = 3\n",
    "args.coord_dim = 2\n",
    "\n",
    "# Sparse settings\n",
    "args.context_ratio = 0.1  # 10% observed\n",
    "args.query_ratio = 0.1    # 10% query for training\n",
    "\n",
    "# Model (reduced size)\n",
    "args.model = 'fnounet2d'\n",
    "args.ch = 64\n",
    "args.ch_mult = [1, 2, 2]\n",
    "args.num_res_blocks = 2\n",
    "args.modes = 16\n",
    "args.dropout = 0.1\n",
    "args.norm = 'group_norm'\n",
    "args.use_pos = True\n",
    "args.use_pointwise_op = True\n",
    "\n",
    "# Diffusion\n",
    "args.ns_method = 'vp_cosine'\n",
    "args.timestep_sampler = 'low_discrepancy'\n",
    "args.disp_method = 'sine'\n",
    "args.sigma_blur_min = 0.05\n",
    "args.sigma_blur_max = 0.25\n",
    "args.gp_type = 'exponential'\n",
    "args.gp_exponent = 2.0\n",
    "args.gp_length_scale = 0.05\n",
    "args.gp_sigma = 1.0\n",
    "\n",
    "# Training\n",
    "args.train_batch_size = 64  # Smaller for notebook\n",
    "args.lr = 0.0002\n",
    "args.weight_decay = 0.0\n",
    "args.num_iterations = 10000  # Shorter for demo\n",
    "args.ema_decay = 0.999\n",
    "args.optimizer = 'adam'\n",
    "args.beta1 = 0.9\n",
    "args.beta2 = 0.999\n",
    "\n",
    "# Logging\n",
    "args.print_every = 50\n",
    "args.save_every = 2000\n",
    "args.vis_every = 500\n",
    "args.eval_every = 5000\n",
    "args.vis_batch_size = 16\n",
    "args.resume = True\n",
    "\n",
    "# Sampling\n",
    "args.num_steps = 250\n",
    "args.sampler = 'denoise'\n",
    "args.s_min = 0.0001\n",
    "\n",
    "# Misc\n",
    "args.distributed = False\n",
    "args.global_rank = 0\n",
    "args.checkpoint_file = 'checkpoint.pt'\n",
    "args.use_clip = False\n",
    "args.weight_method = None\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(args.exp_path, exist_ok=True)\n",
    "os.makedirs(os.path.join(args.exp_path, 'samples'), exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Experiment: {args.exp_path}\")\n",
    "print(f\"  Context ratio: {args.context_ratio*100:.0f}%\")\n",
    "print(f\"  Model: ch={args.ch}, ch_mult={args.ch_mult}\")\n",
    "print(f\"  Iterations: {args.num_iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "base_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=args.data, train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Wrap with sparse dataset\n",
    "sparse_dataset = SparseImageDatasetWrapper(\n",
    "    dataset=base_dataset,\n",
    "    context_ratio=args.context_ratio,\n",
    "    query_ratio=args.query_ratio,\n",
    "    mode='train',\n",
    "    return_full_image=True\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    sparse_dataset,\n",
    "    batch_size=args.train_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {len(sparse_dataset)} images\")\n",
    "print(f\"Context pixels: {sparse_dataset.num_context}\")\n",
    "print(f\"Query pixels: {sparse_dataset.num_query}\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples\n",
    "num_vis = 8\n",
    "originals = []\n",
    "masked = []\n",
    "\n",
    "for i in range(num_vis):\n",
    "    sample = sparse_dataset[i]\n",
    "    originals.append(sample['image'])\n",
    "    masked.append(create_sparse_mask_image(\n",
    "        sample['image'], sample['context_indices'], fill_value=0.5\n",
    "    ))\n",
    "\n",
    "originals = torch.stack(originals)\n",
    "masked = torch.stack(masked)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 6))\n",
    "\n",
    "axes[0].imshow(get_grid_image(originals, nrow=4, to_numpy=True))\n",
    "axes[0].set_title('Original Images')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(get_grid_image(masked, nrow=4, to_numpy=True))\n",
    "axes[1].set_title('10% Observed Pixels (Sparse Input)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## 3. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "gen_sde, optimizer, count, best_loss = init_model(args)\n",
    "\n",
    "# Count parameters\n",
    "num_params = count_parameters_in_M(gen_sde._model)\n",
    "print(f\"Model parameters: {num_params:.2f}M\")\n",
    "print(f\"Starting from iteration: {count}\")\n",
    "print(f\"Best loss: {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-header",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train",
   "metadata": {},
   "outputs": [],
   "source": "# Training\ntorch.manual_seed(args.seed)\nnp.random.seed(args.seed)\n\nwriter = Writer(args.global_rank, args.exp_path)\nstart_time = time.time()\n\ngen_sde.train()\ntrain_iter = iter(train_loader)\npbar = tqdm(total=args.num_iterations, initial=count, desc='Training')\n\nwhile count < args.num_iterations:\n    try:\n        batch = next(train_iter)\n    except StopIteration:\n        train_iter = iter(train_loader)\n        batch = next(train_iter)\n\n    # Get data\n    full_images = batch['image'].cuda()\n    context_coords = batch['context_coords'].cuda()\n    context_values = batch['context_values'].cuda()\n\n    # Get coordinate grid\n    v = get_mgrid(2, args.train_img_height).repeat(full_images.shape[0], 1, 1, 1).cuda()\n\n    # Forward pass\n    optimizer.zero_grad()\n    loss = gen_sde.dsm(full_images, v).mean()\n\n    # Backward\n    loss.backward()\n    optimizer.step()\n\n    count += 1\n    pbar.update(1)\n\n    # Logging\n    if count % args.print_every == 0:\n        elapsed = (time.time() - start_time) / args.print_every\n        lr = optimizer.param_groups[0]['lr']\n        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{lr:.6f}'})\n        writer.add_scalar('train/loss', loss.item(), count)\n        start_time = time.time()\n\n    # Visualization\n    if count % args.vis_every == 0:\n        # Save checkpoint\n        from utils.utils import save_checkpoint\n        \n        save_checkpoint(\n            args, count, loss.item(), gen_sde, optimizer, None, 'checkpoint.pt'\n        )\n        \n        # Visualize\n        num_vis = 8\n        originals_vis = []\n        contexts_vis = []\n\n        for i in range(num_vis):\n            sample = sparse_dataset[i]\n            originals_vis.append(sample['image'])\n            contexts_vis.append(create_sparse_mask_image(\n                sample['image'], sample['context_indices'], fill_value=0.5\n            ))\n\n        originals_vis = torch.stack(originals_vis)\n        contexts_vis = torch.stack(contexts_vis)\n\n        # Save visualization\n        fig_path = os.path.join(args.exp_path, 'samples', f'iter_{count:06d}.png')\n        comparison = torch.cat([contexts_vis, originals_vis], dim=0)\n        torchvision.utils.save_image(\n            comparison, fig_path, nrow=4, padding=2, normalize=True\n        )\n\npbar.close()\nprint('Training completed!')"
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "## 5. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show latest reconstruction\n",
    "from PIL import Image\n",
    "\n",
    "latest_img = os.path.join(args.exp_path, 'samples', f'iter_{count:06d}.png')\n",
    "if os.path.exists(latest_img):\n",
    "    img = Image.open(latest_img)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.title(f'Results at iteration {count}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No visualization found yet\")\n",
    "\n",
    "print(f\"\\nExperiment path: {args.exp_path}\")\n",
    "print(f\"Samples: {os.path.join(args.exp_path, 'samples')}\")\n",
    "print(f\"TensorBoard: tensorboard --logdir={args.exp_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}