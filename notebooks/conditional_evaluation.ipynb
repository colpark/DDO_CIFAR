{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Sparse Reconstruction - Evaluation\n",
    "\n",
    "Comprehensive evaluation with:\n",
    "1. **Conditional Sampling**: Generate full reconstructions from sparse context\n",
    "2. **MSE Evaluation**: Mean Squared Error on reconstructed images\n",
    "3. **CRPS**: Continuous Ranked Probability Score for ensemble evaluation\n",
    "4. **Visualization**: Compare context → reconstruction → ground truth\n",
    "\n",
    "## CRPS Explanation\n",
    "\n",
    "**Continuous Ranked Probability Score** generalizes MAE to probabilistic forecasts:\n",
    "\n",
    "$$\\text{CRPS}(F, y) = \\int_{-\\infty}^{\\infty} [F(x) - H(x - y)]^2 dx$$\n",
    "\n",
    "Where:\n",
    "- $F$ = CDF of your ensemble forecast\n",
    "- $y$ = Ground truth observation\n",
    "- $H$ = Heaviside step function\n",
    "\n",
    "**For ensemble of M samples**:\n",
    "$$\\text{CRPS} = \\frac{1}{M} \\sum_{i=1}^M |x_i - y| - \\frac{1}{2M^2} \\sum_{i=1}^M \\sum_{j=1}^M |x_i - x_j|$$\n",
    "\n",
    "**Key properties**:\n",
    "- Rewards **sharpness** (narrow distribution if correct)\n",
    "- Rewards **calibration** (GT should be plausible from ensemble)\n",
    "- Collapses to MAE for deterministic forecasts (M=1)\n",
    "- Lower is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Change to parent directory\n",
    "os.chdir('..')\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utilities\n",
    "from utils.sparse_datasets_fixed import (\n",
    "    FixedSparseMaskDataset,\n",
    "    create_context_image_batched,\n",
    "    create_sparse_mask_image\n",
    ")\n",
    "from utils.visualize import get_grid_image\n",
    "from utils.utils import Writer, count_parameters_in_M, load_checkpoint\n",
    "\n",
    "# Import DDO components\n",
    "from lib.diffusion import BlurringDiffusion, DenoisingDiffusion, diffuse\n",
    "from lib.models.fourier_unet import FNOUNet2d\n",
    "from lib.conditional_model import ConditionalDDOModel\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration & Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "args = argparse.Namespace()\n",
    "\n",
    "# Paths\n",
    "args.exp_path = './experiments/conditional_sparse_recon'  # Where trained model is\n",
    "args.data = './data'\n",
    "args.seed = 1\n",
    "\n",
    "# Dataset\n",
    "args.dataset = 'cifar10'\n",
    "args.train_img_height = 32\n",
    "args.input_dim = 3\n",
    "args.coord_dim = 2\n",
    "\n",
    "# Sparse settings (must match training)\n",
    "args.context_ratio = 0.1\n",
    "args.query_ratio = 0.1\n",
    "args.mask_seed = 42\n",
    "\n",
    "# Model architecture (must match training)\n",
    "args.model = 'fnounet2d'\n",
    "args.ch = 64\n",
    "args.ch_mult = [1, 2, 2]\n",
    "args.num_res_blocks = 2\n",
    "args.modes = 16\n",
    "args.dropout = 0.1\n",
    "args.norm = 'group_norm'\n",
    "args.use_pos = True\n",
    "args.use_pointwise_op = True\n",
    "args.context_feature_dim = 32\n",
    "\n",
    "# Diffusion settings\n",
    "args.ns_method = 'vp_cosine'\n",
    "args.timestep_sampler = 'low_discrepancy'\n",
    "args.disp_method = 'sine'\n",
    "args.sigma_blur_min = 0.05\n",
    "args.sigma_blur_max = 0.25\n",
    "args.gp_type = 'exponential'\n",
    "args.gp_exponent = 2.0\n",
    "args.gp_length_scale = 0.05\n",
    "args.gp_sigma = 1.0\n",
    "\n",
    "# Sampling\n",
    "args.num_steps = 250\n",
    "args.sampler = 'denoise'\n",
    "args.s_min = 0.0001\n",
    "args.use_clip = False\n",
    "args.weight_method = None\n",
    "\n",
    "# Evaluation settings\n",
    "args.num_eval_samples = 1000  # Number of images to evaluate\n",
    "args.num_ensemble = 10        # Number of samples per image for CRPS\n",
    "args.eval_batch_size = 16     # Batch size for evaluation\n",
    "\n",
    "# Misc\n",
    "args.checkpoint_file = 'checkpoint.pt'\n",
    "args.ema_decay = 0.999\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Evaluation Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model path: {args.exp_path}\")\n",
    "print(f\"Context ratio: {args.context_ratio*100:.0f}%\")\n",
    "print(f\"Query ratio: {args.query_ratio*100:.0f}%\")\n",
    "print(f\"Num eval samples: {args.num_eval_samples}\")\n",
    "print(f\"Ensemble size (for CRPS): {args.num_ensemble}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 test set\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=args.data, train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "print(f\"Test dataset: {len(test_dataset)} images\")\n",
    "\n",
    "# Wrap with fixed sparse masks\n",
    "sparse_test_dataset = FixedSparseMaskDataset(\n",
    "    dataset=test_dataset,\n",
    "    context_ratio=args.context_ratio,\n",
    "    query_ratio=args.query_ratio,\n",
    "    seed=args.mask_seed + 1000  # Different seed from training\n",
    ")\n",
    "\n",
    "print(sparse_test_dataset)\n",
    "\n",
    "# Create dataloader\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    sparse_test_dataset,\n",
    "    batch_size=args.eval_batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nTest loader: {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mgrid(dim, img_height):\n",
    "    \"\"\"Generate coordinate grid\"\"\"\n",
    "    grid = torch.linspace(0, img_height-1, img_height) / img_height\n",
    "    if dim == 2:\n",
    "        grid = torch.cat([grid[None,None,...,None].repeat(1, 1, 1, img_height),\n",
    "                          grid[None,None,None].repeat(1, 1, img_height, 1)], dim=1)\n",
    "    return grid\n",
    "\n",
    "\n",
    "def load_trained_model(args):\n",
    "    \"\"\"Load trained conditional DDO model\"\"\"\n",
    "    \n",
    "    # GP config\n",
    "    gp_config = argparse.Namespace()\n",
    "    gp_config.device = 'cuda'\n",
    "    gp_config.exponent = args.gp_exponent\n",
    "    gp_config.length_scale = args.gp_length_scale\n",
    "    gp_config.sigma = args.gp_sigma\n",
    "    \n",
    "    # Blurring config\n",
    "    disp_config = argparse.Namespace()\n",
    "    disp_config.sigma_blur_min = args.sigma_blur_min\n",
    "    disp_config.sigma_blur_max = args.sigma_blur_max\n",
    "    \n",
    "    # Create diffusion process\n",
    "    inf_sde = BlurringDiffusion(\n",
    "        dim=args.coord_dim,\n",
    "        ch=args.input_dim,\n",
    "        ns_method=args.ns_method,\n",
    "        disp_method=args.disp_method,\n",
    "        disp_config=disp_config,\n",
    "        gp_type=args.gp_type,\n",
    "        gp_config=gp_config,\n",
    "    )\n",
    "    \n",
    "    # Create base FNO-UNet\n",
    "    base_unet = FNOUNet2d(\n",
    "        modes_height=args.modes,\n",
    "        modes_width=args.modes,\n",
    "        in_channels=args.input_dim,\n",
    "        in_height=args.train_img_height,\n",
    "        ch=args.ch,\n",
    "        ch_mult=tuple(args.ch_mult),\n",
    "        num_res_blocks=args.num_res_blocks,\n",
    "        dropout=args.dropout,\n",
    "        norm=args.norm,\n",
    "        use_pos=args.use_pos,\n",
    "        use_pointwise_op=args.use_pointwise_op,\n",
    "    )\n",
    "    \n",
    "    # Wrap with conditional layer\n",
    "    model = ConditionalDDOModel(\n",
    "        base_unet,\n",
    "        input_dim=args.input_dim,\n",
    "        context_feature_dim=args.context_feature_dim\n",
    "    )\n",
    "    \n",
    "    # Create denoising diffusion wrapper\n",
    "    gen_sde = DenoisingDiffusion(\n",
    "        inf_sde,\n",
    "        model=model,\n",
    "        timestep_sampler=args.timestep_sampler,\n",
    "        use_clip=args.use_clip,\n",
    "        weight_method=args.weight_method\n",
    "    ).cuda()\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint_file = os.path.join(args.exp_path, args.checkpoint_file)\n",
    "    if not os.path.exists(checkpoint_file):\n",
    "        raise FileNotFoundError(f\"No checkpoint found at {checkpoint_file}\")\n",
    "    \n",
    "    print(f\"Loading checkpoint from {checkpoint_file}\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location='cuda')\n",
    "    gen_sde.load_state_dict(checkpoint['gen_sde_state_dict'])\n",
    "    \n",
    "    # Use EMA weights if available\n",
    "    if 'gen_sde_optimizer' in checkpoint and 'ema' in checkpoint['gen_sde_optimizer']:\n",
    "        print(\"Loading EMA weights\")\n",
    "        gen_sde.load_state_dict(checkpoint['gen_sde_optimizer']['ema'])\n",
    "    \n",
    "    iteration = checkpoint.get('global_step', 0)\n",
    "    print(f\"Loaded model from iteration {iteration}\")\n",
    "    \n",
    "    gen_sde.eval()\n",
    "    return gen_sde\n",
    "\n",
    "\n",
    "# Load model\n",
    "gen_sde = load_trained_model(args)\n",
    "\n",
    "num_params = count_parameters_in_M(gen_sde._model)\n",
    "print(f\"\\nModel parameters: {num_params:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conditional Sampling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_conditional(gen_sde, context_image, v, num_steps=250, sampler='denoise'):\n",
    "    \"\"\"\n",
    "    Generate a sample conditioned on sparse context.\n",
    "    \n",
    "    Args:\n",
    "        gen_sde: Trained conditional diffusion model\n",
    "        context_image: (B, C, H, W) sparse observations\n",
    "        v: (B, coord_dim, H, W) coordinate grid\n",
    "        num_steps: Number of diffusion steps\n",
    "        sampler: Sampling method\n",
    "    \n",
    "    Returns:\n",
    "        Generated samples (B, C, H, W)\n",
    "    \"\"\"\n",
    "    batch_size = context_image.shape[0]\n",
    "    device = context_image.device\n",
    "    \n",
    "    # Start from random noise\n",
    "    x_T = torch.randn_like(context_image)\n",
    "    \n",
    "    # Run reverse diffusion conditioned on context\n",
    "    samples = diffuse(\n",
    "        gen_sde,\n",
    "        num_steps=num_steps,\n",
    "        x_0=x_T,\n",
    "        v=v,\n",
    "        sampler=sampler,\n",
    "        context_image=context_image,  # Pass context!\n",
    "        disable_tqdm=False\n",
    "    )\n",
    "    \n",
    "    return samples[-1]  # Return final sample\n",
    "\n",
    "\n",
    "def sample_ensemble_conditional(gen_sde, context_image, v, num_ensemble=10, num_steps=250):\n",
    "    \"\"\"\n",
    "    Generate multiple samples (ensemble) for CRPS evaluation.\n",
    "    \n",
    "    Returns:\n",
    "        Ensemble of shape (num_ensemble, B, C, H, W)\n",
    "    \"\"\"\n",
    "    ensemble = []\n",
    "    \n",
    "    for i in tqdm(range(num_ensemble), desc='Generating ensemble'):\n",
    "        sample = sample_conditional(gen_sde, context_image, v, num_steps)\n",
    "        ensemble.append(sample)\n",
    "    \n",
    "    return torch.stack(ensemble, dim=0)  # (M, B, C, H, W)\n",
    "\n",
    "\n",
    "print(\"Sampling functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(predictions, ground_truth):\n",
    "    \"\"\"\n",
    "    Compute Mean Squared Error.\n",
    "    \n",
    "    Args:\n",
    "        predictions: (B, C, H, W)\n",
    "        ground_truth: (B, C, H, W)\n",
    "    \n",
    "    Returns:\n",
    "        MSE per image (B,) and overall mean\n",
    "    \"\"\"\n",
    "    mse_per_image = ((predictions - ground_truth) ** 2).mean(dim=(1, 2, 3))\n",
    "    return mse_per_image, mse_per_image.mean()\n",
    "\n",
    "\n",
    "def compute_mae(predictions, ground_truth):\n",
    "    \"\"\"\n",
    "    Compute Mean Absolute Error.\n",
    "    \n",
    "    Returns:\n",
    "        MAE per image (B,) and overall mean\n",
    "    \"\"\"\n",
    "    mae_per_image = (predictions - ground_truth).abs().mean(dim=(1, 2, 3))\n",
    "    return mae_per_image, mae_per_image.mean()\n",
    "\n",
    "\n",
    "def compute_crps_ensemble(ensemble, ground_truth):\n",
    "    \"\"\"\n",
    "    Compute Continuous Ranked Probability Score for ensemble forecasts.\n",
    "    \n",
    "    Args:\n",
    "        ensemble: (M, B, C, H, W) - M ensemble members\n",
    "        ground_truth: (B, C, H, W)\n",
    "    \n",
    "    Returns:\n",
    "        CRPS per image (B,) and overall mean\n",
    "    \n",
    "    Formula:\n",
    "        CRPS = (1/M) * sum_i |x_i - y| - (1/2M^2) * sum_i sum_j |x_i - x_j|\n",
    "    \n",
    "    Where:\n",
    "        x_i = ensemble member i\n",
    "        y = ground truth\n",
    "        M = ensemble size\n",
    "    \n",
    "    Properties:\n",
    "        - Rewards sharpness (narrow distribution if correct)\n",
    "        - Rewards calibration (GT should be plausible)\n",
    "        - Collapses to MAE when M=1\n",
    "        - Lower is better\n",
    "    \"\"\"\n",
    "    M = ensemble.shape[0]  # Ensemble size\n",
    "    B = ground_truth.shape[0]  # Batch size\n",
    "    \n",
    "    # Flatten spatial dimensions for easier computation\n",
    "    # ensemble: (M, B, C*H*W)\n",
    "    # ground_truth: (B, C*H*W)\n",
    "    ensemble_flat = ensemble.reshape(M, B, -1)\n",
    "    gt_flat = ground_truth.reshape(B, -1)\n",
    "    \n",
    "    # Term 1: Average distance from each ensemble member to ground truth\n",
    "    # |x_i - y| averaged over ensemble\n",
    "    term1 = torch.abs(ensemble_flat - gt_flat[None, :, :]).mean(dim=0)  # (B, C*H*W)\n",
    "    \n",
    "    # Term 2: Average pairwise distance between ensemble members (sharpness penalty)\n",
    "    # |x_i - x_j| averaged over all pairs\n",
    "    term2 = 0.0\n",
    "    for i in range(M):\n",
    "        for j in range(M):\n",
    "            term2 += torch.abs(ensemble_flat[i] - ensemble_flat[j])  # (B, C*H*W)\n",
    "    term2 = term2 / (2 * M * M)\n",
    "    \n",
    "    # CRPS per pixel, then average over pixels\n",
    "    crps_per_pixel = term1 - term2  # (B, C*H*W)\n",
    "    crps_per_image = crps_per_pixel.mean(dim=1)  # (B,)\n",
    "    \n",
    "    return crps_per_image, crps_per_image.mean()\n",
    "\n",
    "\n",
    "def compute_psnr(predictions, ground_truth, max_val=1.0):\n",
    "    \"\"\"\n",
    "    Compute Peak Signal-to-Noise Ratio.\n",
    "    \n",
    "    Returns:\n",
    "        PSNR per image (B,) and overall mean\n",
    "    \"\"\"\n",
    "    mse_per_image, _ = compute_mse(predictions, ground_truth)\n",
    "    psnr_per_image = 20 * torch.log10(max_val / torch.sqrt(mse_per_image))\n",
    "    return psnr_per_image, psnr_per_image.mean()\n",
    "\n",
    "\n",
    "print(\"Evaluation metrics defined!\")\n",
    "print(\"\\nAvailable metrics:\")\n",
    "print(\"  - MSE: Mean Squared Error\")\n",
    "print(\"  - MAE: Mean Absolute Error (deterministic baseline)\")\n",
    "print(\"  - CRPS: Continuous Ranked Probability Score (ensemble)\")\n",
    "print(\"  - PSNR: Peak Signal-to-Noise Ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Evaluation\n",
    "\n",
    "### Quick Test: Single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a single image first\n",
    "test_sample = sparse_test_dataset[0]\n",
    "\n",
    "test_image = test_sample['image'].unsqueeze(0).cuda()  # (1, 3, 32, 32)\n",
    "test_context_indices = test_sample['context_indices'].unsqueeze(0).cuda()\n",
    "test_context_values = test_sample['context_values'].unsqueeze(0).cuda()\n",
    "\n",
    "# Create context image\n",
    "test_context_image = create_context_image_batched(\n",
    "    test_context_values,\n",
    "    test_context_indices,\n",
    "    32, 32, 3\n",
    ")\n",
    "\n",
    "# Coordinate grid\n",
    "v_grid = get_mgrid(2, 32).cuda()\n",
    "\n",
    "print(\"Generating single conditional sample...\")\n",
    "reconstruction = sample_conditional(\n",
    "    gen_sde,\n",
    "    test_context_image,\n",
    "    v_grid,\n",
    "    num_steps=args.num_steps\n",
    ")\n",
    "\n",
    "# Compute metrics\n",
    "_, mse = compute_mse(reconstruction, test_image)\n",
    "_, psnr = compute_psnr(reconstruction, test_image)\n",
    "_, mae = compute_mae(reconstruction, test_image)\n",
    "\n",
    "print(f\"\\nSingle sample metrics:\")\n",
    "print(f\"  MSE:  {mse:.6f}\")\n",
    "print(f\"  MAE:  {mae:.6f}\")\n",
    "print(f\"  PSNR: {psnr:.2f} dB\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# Context\n",
    "context_vis = create_sparse_mask_image(\n",
    "    test_image[0].cpu(), test_sample['context_indices'], fill_value=0.5\n",
    ")\n",
    "axes[0].imshow(context_vis.permute(1, 2, 0).numpy())\n",
    "axes[0].set_title(f'Context (10%)', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Reconstruction\n",
    "axes[1].imshow(reconstruction[0].cpu().permute(1, 2, 0).clamp(0, 1).numpy())\n",
    "axes[1].set_title(f'Reconstruction\\nMSE={mse:.4f}, PSNR={psnr:.1f}dB', fontsize=12)\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Ground truth\n",
    "axes[2].imshow(test_image[0].cpu().permute(1, 2, 0).numpy())\n",
    "axes[2].set_title('Ground Truth', fontsize=12)\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Evaluation: Multiple Images with Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full evaluation with ensemble for CRPS\n",
    "num_eval = min(args.num_eval_samples, len(sparse_test_dataset))\n",
    "print(f\"Evaluating on {num_eval} images...\")\n",
    "print(f\"Ensemble size: {args.num_ensemble}\")\n",
    "print(f\"This will take a while...\\n\")\n",
    "\n",
    "all_mse = []\n",
    "all_mae = []\n",
    "all_psnr = []\n",
    "all_crps = []\n",
    "\n",
    "# Coordinate grid (shared)\n",
    "v_grid = get_mgrid(2, 32).cuda()\n",
    "\n",
    "for idx in tqdm(range(0, num_eval, args.eval_batch_size), desc='Evaluation'):\n",
    "    batch_end = min(idx + args.eval_batch_size, num_eval)\n",
    "    batch_indices = range(idx, batch_end)\n",
    "    \n",
    "    # Gather batch\n",
    "    batch_images = []\n",
    "    batch_context_indices = []\n",
    "    batch_context_values = []\n",
    "    \n",
    "    for i in batch_indices:\n",
    "        sample = sparse_test_dataset[i]\n",
    "        batch_images.append(sample['image'])\n",
    "        batch_context_indices.append(sample['context_indices'])\n",
    "        batch_context_values.append(sample['context_values'])\n",
    "    \n",
    "    batch_images = torch.stack(batch_images).cuda()\n",
    "    batch_context_indices = torch.stack(batch_context_indices).cuda()\n",
    "    batch_context_values = torch.stack(batch_context_values).cuda()\n",
    "    \n",
    "    batch_size = batch_images.shape[0]\n",
    "    \n",
    "    # Create context image\n",
    "    context_image = create_context_image_batched(\n",
    "        batch_context_values,\n",
    "        batch_context_indices,\n",
    "        32, 32, 3\n",
    "    )\n",
    "    \n",
    "    v_batch = v_grid.repeat(batch_size, 1, 1, 1)\n",
    "    \n",
    "    # Generate ensemble\n",
    "    ensemble = sample_ensemble_conditional(\n",
    "        gen_sde,\n",
    "        context_image,\n",
    "        v_batch,\n",
    "        num_ensemble=args.num_ensemble,\n",
    "        num_steps=args.num_steps\n",
    "    )  # (M, B, C, H, W)\n",
    "    \n",
    "    # Use mean of ensemble as point estimate\n",
    "    mean_prediction = ensemble.mean(dim=0)  # (B, C, H, W)\n",
    "    \n",
    "    # Compute metrics\n",
    "    mse_batch, _ = compute_mse(mean_prediction, batch_images)\n",
    "    mae_batch, _ = compute_mae(mean_prediction, batch_images)\n",
    "    psnr_batch, _ = compute_psnr(mean_prediction, batch_images)\n",
    "    crps_batch, _ = compute_crps_ensemble(ensemble, batch_images)\n",
    "    \n",
    "    all_mse.extend(mse_batch.cpu().numpy())\n",
    "    all_mae.extend(mae_batch.cpu().numpy())\n",
    "    all_psnr.extend(psnr_batch.cpu().numpy())\n",
    "    all_crps.extend(crps_batch.cpu().numpy())\n",
    "\n",
    "# Convert to arrays\n",
    "all_mse = np.array(all_mse)\n",
    "all_mae = np.array(all_mae)\n",
    "all_psnr = np.array(all_psnr)\n",
    "all_crps = np.array(all_crps)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Evaluated on {len(all_mse)} images\")\n",
    "print(f\"Ensemble size: {args.num_ensemble}\\n\")\n",
    "\n",
    "print(\"Point Estimate Metrics (using ensemble mean):\")\n",
    "print(f\"  MSE:  {all_mse.mean():.6f} ± {all_mse.std():.6f}\")\n",
    "print(f\"  MAE:  {all_mae.mean():.6f} ± {all_mae.std():.6f}\")\n",
    "print(f\"  PSNR: {all_psnr.mean():.2f} ± {all_psnr.std():.2f} dB\\n\")\n",
    "\n",
    "print(\"Probabilistic Metric:\")\n",
    "print(f\"  CRPS: {all_crps.mean():.6f} ± {all_crps.std():.6f}\")\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  MAE (deterministic baseline): {all_mae.mean():.6f}\")\n",
    "print(f\"  CRPS (ensemble):              {all_crps.mean():.6f}\")\n",
    "print(f\"  Ratio (CRPS/MAE):             {all_crps.mean()/all_mae.mean():.3f}\")\n",
    "\n",
    "if all_crps.mean() < all_mae.mean():\n",
    "    print(\"  → CRPS < MAE: Ensemble is well-calibrated and sharp!\")\n",
    "else:\n",
    "    print(\"  → CRPS > MAE: Ensemble may be too spread out or miscalibrated\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metric distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "axes[0, 0].hist(all_mse, bins=50, alpha=0.7, color='blue')\n",
    "axes[0, 0].axvline(all_mse.mean(), color='red', linestyle='--', label=f'Mean: {all_mse.mean():.4f}')\n",
    "axes[0, 0].set_xlabel('MSE')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('MSE Distribution')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "axes[0, 1].hist(all_psnr, bins=50, alpha=0.7, color='green')\n",
    "axes[0, 1].axvline(all_psnr.mean(), color='red', linestyle='--', label=f'Mean: {all_psnr.mean():.2f} dB')\n",
    "axes[0, 1].set_xlabel('PSNR (dB)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('PSNR Distribution')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "axes[1, 0].hist(all_mae, bins=50, alpha=0.7, color='orange', label='MAE')\n",
    "axes[1, 0].axvline(all_mae.mean(), color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Error')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('MAE Distribution (Deterministic Baseline)')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "axes[1, 1].hist(all_crps, bins=50, alpha=0.7, color='purple', label='CRPS')\n",
    "axes[1, 1].axvline(all_crps.mean(), color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 1].hist(all_mae, bins=50, alpha=0.3, color='orange', label='MAE (for comparison)')\n",
    "axes[1, 1].set_xlabel('Error')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('CRPS vs MAE Distribution')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(args.exp_path, 'evaluation_metrics.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved metrics plot to {args.exp_path}/evaluation_metrics.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Best and Worst Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best and worst by MSE\n",
    "best_indices = np.argsort(all_mse)[:8]  # 8 best\n",
    "worst_indices = np.argsort(all_mse)[-8:]  # 8 worst\n",
    "\n",
    "print(f\"Best MSE: {all_mse[best_indices[0]]:.6f}\")\n",
    "print(f\"Worst MSE: {all_mse[worst_indices[-1]]:.6f}\")\n",
    "\n",
    "# Regenerate for visualization\n",
    "# TODO: Add visualization code here\n",
    "\n",
    "print(\"\\nEvaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results = {\n",
    "    'num_samples': len(all_mse),\n",
    "    'ensemble_size': args.num_ensemble,\n",
    "    'mse_mean': float(all_mse.mean()),\n",
    "    'mse_std': float(all_mse.std()),\n",
    "    'mae_mean': float(all_mae.mean()),\n",
    "    'mae_std': float(all_mae.std()),\n",
    "    'psnr_mean': float(all_psnr.mean()),\n",
    "    'psnr_std': float(all_psnr.std()),\n",
    "    'crps_mean': float(all_crps.mean()),\n",
    "    'crps_std': float(all_crps.std()),\n",
    "    'crps_to_mae_ratio': float(all_crps.mean() / all_mae.mean()),\n",
    "}\n",
    "\n",
    "import json\n",
    "results_file = os.path.join(args.exp_path, 'evaluation_results.json')\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {results_file}\")\n",
    "print(\"\\n\" + json.dumps(results, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
