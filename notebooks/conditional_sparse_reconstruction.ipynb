{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Sparse Image Reconstruction with DDO\n",
    "\n",
    "## Task\n",
    "Train a **conditional diffusion model** for sparse image reconstruction:\n",
    "- Each image has a **fixed** sparse mask (doesn't change during training)\n",
    "- 20% total allowance: **10% context** (input) + **10% query** (ground truth)\n",
    "- Model learns: `context (10%) → full image`, trained on query pixels\n",
    "\n",
    "## Key Features\n",
    "1. **Fixed masks per instance** - Each of 60K images has same mask every epoch\n",
    "2. **Conditional generation** - Model sees context as input\n",
    "3. **Query-based loss** - Loss computed only on 10% query pixels (or full image)\n",
    "4. **DDO framework** - Uses function-space diffusion with GP noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
    "display(HTML(\"<style>.output_result { max-width:98% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Change to parent directory\n",
    "os.chdir('..')\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utilities\n",
    "from utils.sparse_datasets_fixed import (\n",
    "    FixedSparseMaskDataset,\n",
    "    create_context_image_batched,\n",
    "    create_sparse_mask_image\n",
    ")\n",
    "from utils.visualize import get_grid_image\n",
    "from utils.utils import Writer, count_parameters_in_M, save_checkpoint, load_checkpoint\n",
    "from utils.ema import EMA\n",
    "\n",
    "# Import DDO components\n",
    "from lib.diffusion import BlurringDiffusion, DenoisingDiffusion\n",
    "from lib.models.fourier_unet import FNOUNet2d\n",
    "from lib.conditional_model import ConditionalDDOModel, ConditionalDDOModelSimple\n",
    "\n",
    "# Re-import tqdm after main imports\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "args = argparse.Namespace()\n",
    "\n",
    "# Paths\n",
    "args.exp_path = './experiments/conditional_sparse_recon'\n",
    "args.data = './data'\n",
    "args.seed = 1\n",
    "\n",
    "# Dataset\n",
    "args.dataset = 'cifar10'\n",
    "args.train_img_height = 32\n",
    "args.input_dim = 3\n",
    "args.coord_dim = 2\n",
    "\n",
    "# Sparse conditioning settings\n",
    "args.context_ratio = 0.1   # 10% for context (input)\n",
    "args.query_ratio = 0.1     # 10% for query (GT target)\n",
    "args.mask_seed = 42        # Seed for fixed masks\n",
    "\n",
    "# Model architecture\n",
    "args.model = 'fnounet2d'\n",
    "args.ch = 64                    # Base channels\n",
    "args.ch_mult = [1, 2, 2]        # Channel multipliers\n",
    "args.num_res_blocks = 2         # Residual blocks per level\n",
    "args.modes = 16                 # Fourier modes\n",
    "args.dropout = 0.1\n",
    "args.norm = 'group_norm'\n",
    "args.use_pos = True\n",
    "args.use_pointwise_op = True\n",
    "args.context_feature_dim = 32   # Context encoder output channels\n",
    "args.use_simple_conditioning = False  # True = simple concatenation, False = encoder\n",
    "\n",
    "# Diffusion settings (function-space DDO)\n",
    "args.ns_method = 'vp_cosine'\n",
    "args.timestep_sampler = 'low_discrepancy'\n",
    "args.disp_method = 'sine'\n",
    "args.sigma_blur_min = 0.05\n",
    "args.sigma_blur_max = 0.25\n",
    "args.gp_type = 'exponential'\n",
    "args.gp_exponent = 2.0\n",
    "args.gp_length_scale = 0.05\n",
    "args.gp_sigma = 1.0\n",
    "\n",
    "# Training\n",
    "args.train_batch_size = 128\n",
    "args.lr = 0.0002\n",
    "args.weight_decay = 0.0\n",
    "args.num_iterations = 100000\n",
    "args.ema_decay = 0.999\n",
    "args.optimizer = 'adam'\n",
    "args.beta1 = 0.9\n",
    "args.beta2 = 0.999\n",
    "\n",
    "# Logging\n",
    "args.print_every = 100\n",
    "args.save_every = 5000\n",
    "args.vis_every = 1000\n",
    "args.vis_batch_size = 16\n",
    "args.resume = True\n",
    "\n",
    "# Sampling\n",
    "args.num_steps = 250\n",
    "args.sampler = 'denoise'\n",
    "args.s_min = 0.0001\n",
    "\n",
    "# Misc\n",
    "args.distributed = False\n",
    "args.global_rank = 0\n",
    "args.checkpoint_file = 'checkpoint.pt'\n",
    "args.use_clip = False\n",
    "args.weight_method = None\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(args.exp_path, exist_ok=True)\n",
    "os.makedirs(os.path.join(args.exp_path, 'samples'), exist_ok=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Conditional Sparse Reconstruction Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Experiment: {args.exp_path}\")\n",
    "print(f\"Context ratio: {args.context_ratio*100:.0f}% (input)\")\n",
    "print(f\"Query ratio: {args.query_ratio*100:.0f}% (GT target)\")\n",
    "print(f\"Model: ch={args.ch}, ch_mult={args.ch_mult}, modes={args.modes}\")\n",
    "print(f\"Conditioning: {'Simple' if args.use_simple_conditioning else 'With Encoder'}\")\n",
    "print(f\"Iterations: {args.num_iterations}\")\n",
    "print(f\"Batch size: {args.train_batch_size}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset with Fixed Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "base_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=args.data, train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "print(f\"Base dataset: {len(base_dataset)} images\")\n",
    "\n",
    "# Wrap with fixed sparse masks\n",
    "sparse_dataset = FixedSparseMaskDataset(\n",
    "    dataset=base_dataset,\n",
    "    context_ratio=args.context_ratio,\n",
    "    query_ratio=args.query_ratio,\n",
    "    seed=args.mask_seed\n",
    ")\n",
    "\n",
    "print(sparse_dataset)\n",
    "\n",
    "# Create dataloader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    sparse_dataset,\n",
    "    batch_size=args.train_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDataloader: {len(train_loader)} batches per epoch\")\n",
    "print(f\"Total pixels: 32×32 = 1024\")\n",
    "print(f\"Context pixels: {sparse_dataset.num_context} (input)\")\n",
    "print(f\"Query pixels: {sparse_dataset.num_query} (GT target)\")\n",
    "print(f\"Remaining: {1024 - sparse_dataset.num_context - sparse_dataset.num_query} (not used)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Fixed Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples\n",
    "num_vis = 8\n",
    "originals = []\n",
    "contexts = []\n",
    "queries = []\n",
    "\n",
    "for i in range(num_vis):\n",
    "    sample = sparse_dataset[i]\n",
    "    originals.append(sample['image'])\n",
    "    \n",
    "    # Context visualization\n",
    "    contexts.append(create_sparse_mask_image(\n",
    "        sample['image'], sample['context_indices'], fill_value=0.5\n",
    "    ))\n",
    "    \n",
    "    # Query visualization\n",
    "    queries.append(create_sparse_mask_image(\n",
    "        sample['image'], sample['query_indices'], fill_value=0.5\n",
    "    ))\n",
    "\n",
    "originals = torch.stack(originals)\n",
    "contexts = torch.stack(contexts)\n",
    "queries = torch.stack(queries)\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "axes[0].imshow(get_grid_image(originals, nrow=4, to_numpy=True))\n",
    "axes[0].set_title('Original Images', fontsize=14)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(get_grid_image(contexts, nrow=4, to_numpy=True))\n",
    "axes[1].set_title('Context (10% Input) - FIXED per image', fontsize=14)\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(get_grid_image(queries, nrow=4, to_numpy=True))\n",
    "axes[2].set_title('Query (10% GT Target) - FIXED per image', fontsize=14)\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: Context and Query masks are FIXED for each image throughout training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Model with Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mgrid(dim, img_height):\n",
    "    \"\"\"Generate coordinate grid\"\"\"\n",
    "    grid = torch.linspace(0, img_height-1, img_height) / img_height\n",
    "    if dim == 2:\n",
    "        grid = torch.cat([grid[None,None,...,None].repeat(1, 1, 1, img_height),\n",
    "                          grid[None,None,None].repeat(1, 1, img_height, 1)], dim=1)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return grid\n",
    "\n",
    "\n",
    "def init_conditional_model(args):\n",
    "    \"\"\"Initialize conditional DDO model\"\"\"\n",
    "    \n",
    "    # GP config for function-space noise\n",
    "    gp_config = argparse.Namespace()\n",
    "    gp_config.device = 'cuda'\n",
    "    gp_config.exponent = args.gp_exponent\n",
    "    gp_config.length_scale = args.gp_length_scale\n",
    "    gp_config.sigma = args.gp_sigma\n",
    "    \n",
    "    # Blurring config\n",
    "    disp_config = argparse.Namespace()\n",
    "    disp_config.sigma_blur_min = args.sigma_blur_min\n",
    "    disp_config.sigma_blur_max = args.sigma_blur_max\n",
    "    \n",
    "    # Create diffusion process (function-space)\n",
    "    inf_sde = BlurringDiffusion(\n",
    "        dim=args.coord_dim,\n",
    "        ch=args.input_dim,\n",
    "        ns_method=args.ns_method,\n",
    "        disp_method=args.disp_method,\n",
    "        disp_config=disp_config,\n",
    "        gp_type=args.gp_type,\n",
    "        gp_config=gp_config,\n",
    "    )\n",
    "    \n",
    "    # Create base FNO-UNet\n",
    "    base_unet = FNOUNet2d(\n",
    "        modes_height=args.modes,\n",
    "        modes_width=args.modes,\n",
    "        in_channels=args.input_dim,\n",
    "        in_height=args.train_img_height,\n",
    "        ch=args.ch,\n",
    "        ch_mult=tuple(args.ch_mult),\n",
    "        num_res_blocks=args.num_res_blocks,\n",
    "        dropout=args.dropout,\n",
    "        norm=args.norm,\n",
    "        use_pos=args.use_pos,\n",
    "        use_pointwise_op=args.use_pointwise_op,\n",
    "    )\n",
    "    \n",
    "    # Wrap with conditional layer\n",
    "    if args.use_simple_conditioning:\n",
    "        print(\"Using simple conditioning (concatenation only)\")\n",
    "        model = ConditionalDDOModelSimple(base_unet, input_dim=args.input_dim)\n",
    "    else:\n",
    "        print(\"Using conditioning with encoder\")\n",
    "        model = ConditionalDDOModel(\n",
    "            base_unet,\n",
    "            input_dim=args.input_dim,\n",
    "            context_feature_dim=args.context_feature_dim\n",
    "        )\n",
    "    \n",
    "    # Create denoising diffusion wrapper\n",
    "    gen_sde = DenoisingDiffusion(\n",
    "        inf_sde,\n",
    "        model=model,\n",
    "        timestep_sampler=args.timestep_sampler,\n",
    "        use_clip=args.use_clip,\n",
    "        weight_method=args.weight_method\n",
    "    ).cuda()\n",
    "    \n",
    "    # Optimizer\n",
    "    if args.optimizer == \"adam\":\n",
    "        optimizer = torch.optim.Adam(\n",
    "            gen_sde.parameters(),\n",
    "            lr=args.lr,\n",
    "            betas=(args.beta1, args.beta2),\n",
    "            weight_decay=args.weight_decay\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {args.optimizer}\")\n",
    "    \n",
    "    # Add EMA\n",
    "    optimizer = EMA(optimizer, ema_decay=args.ema_decay)\n",
    "    \n",
    "    # Resume from checkpoint\n",
    "    count = 0\n",
    "    best_loss = 1e10\n",
    "    checkpoint_file = os.path.join(args.exp_path, args.checkpoint_file)\n",
    "    if args.resume and os.path.exists(checkpoint_file):\n",
    "        print(f'Loading checkpoint from {checkpoint_file}')\n",
    "        gen_sde, optimizer, _, count, best_loss = load_checkpoint(\n",
    "            checkpoint_file, gen_sde, optimizer, None\n",
    "        )\n",
    "        print(f'Resumed from iteration {count}')\n",
    "    \n",
    "    return gen_sde, optimizer, count, best_loss\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "gen_sde, optimizer, count, best_loss = init_conditional_model(args)\n",
    "\n",
    "# Count parameters\n",
    "num_params = count_parameters_in_M(gen_sde._model)\n",
    "print(f\"\\nModel parameters: {num_params:.2f}M\")\n",
    "print(f\"Starting from iteration: {count}\")\n",
    "print(f\"Best loss: {best_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Conditional Training Loop - CORRECTED\n\nKey differences from standard training:\n1. Fixed masks per image (same context every epoch)\n2. Context image passed to model during denoising\n3. **Loss computed ONLY on query pixels (10%)** - Model predicts full field but is supervised only on query pixels"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CORRECTED Training Loop - Query-Only Loss\n# Loss computed ONLY on query pixels (10%), not full image\n\nfrom utils.sparse_datasets_fixed import create_query_mask_batched\nfrom lib.diffusion import low_discrepancy_rand\n\ntorch.manual_seed(args.seed)\nnp.random.seed(args.seed)\n\nwriter = Writer(args.global_rank, args.exp_path)\nstart_time = time.time()\n\ngen_sde.train()\ntrain_iter = iter(train_loader)\npbar = tqdm(total=args.num_iterations, initial=count, desc='Conditional Training')\n\n# Pre-compute coordinate grid\nv_grid = get_mgrid(2, args.train_img_height).cuda()\n\nprint(\"=\"*60)\nprint(\"Training with Query-Only Loss\")\nprint(\"=\"*60)\nprint(\"Context (10%): Input condition (fixed per instance)\")\nprint(\"Query (10%): Supervision signal (fixed per instance)\")\nprint(\"Target: Predict full field (100%), supervised only on query\")\nprint(\"=\"*60)\n\nwhile count < args.num_iterations:\n    try:\n        batch = next(train_iter)\n    except StopIteration:\n        train_iter = iter(train_loader)\n        batch = next(train_iter)\n\n    # Get data\n    full_images = batch['image'].cuda()  # (B, C, H, W)\n    context_indices = batch['context_indices'].cuda()  # (B, num_context)\n    context_values = batch['context_values'].cuda()  # (B, num_context, C)\n    query_indices = batch['query_indices'].cuda()  # (B, num_query)\n    query_values = batch['query_values'].cuda()  # (B, num_query, C)\n\n    batch_size = full_images.shape[0]\n\n    # Create context image (sparse observations as dense image)\n    context_image = create_context_image_batched(\n        context_values,\n        context_indices,\n        height=args.train_img_height,\n        width=args.train_img_height,\n        num_channels=args.input_dim\n    )\n\n    # Create query mask for loss computation\n    query_mask = create_query_mask_batched(\n        query_indices,\n        height=args.train_img_height,\n        width=args.train_img_height\n    )  # (B, 1, H, W) - binary mask where 1 = query pixel\n\n    # Coordinate grid\n    v = v_grid.repeat(batch_size, 1, 1, 1)\n\n    # Forward pass with conditioning\n    optimizer.zero_grad()\n\n    # Sample timestep\n    if gen_sde.timestep_sampler == \"uniform\":\n        s_ = torch.rand(batch_size, device=full_images.device) * gen_sde.T\n    elif gen_sde.timestep_sampler == \"low_discrepancy\":\n        s_ = low_discrepancy_rand(batch_size, device=full_images.device) * gen_sde.T\n\n    # Add noise to full image\n    zt, target, _, _ = gen_sde.forward_diffusion.sample(t=s_, x0=full_images)\n\n    # Predict noise conditioned on context (model predicts ALL pixels)\n    pred = gen_sde.epsilon(y=zt, s=s_, v=v, context_image=context_image)\n\n    # Compute MSE between predicted and true noise (all pixels)\n    mse = 0.5 * ((pred - target) ** 2)  # (B, C, H, W)\n\n    # CRITICAL: Only compute loss on QUERY pixels (10%)\n    masked_mse = mse * query_mask  # (B, C, H, W) * (B, 1, H, W)\n\n    # Normalize by number of query pixels\n    num_query_pixels = query_mask.sum(dim=(1, 2, 3), keepdim=True)  # (B, 1, 1, 1)\n    loss_per_sample = masked_mse.sum(dim=(1, 2, 3)) / num_query_pixels.squeeze()  # (B,)\n    loss = loss_per_sample.mean()  # Scalar\n\n    # Backward\n    loss.backward()\n    optimizer.step()\n\n    count += 1\n    pbar.update(1)\n\n    # Logging\n    if count % args.print_every == 0:\n        elapsed = (time.time() - start_time) / args.print_every\n        lr = optimizer.param_groups[0]['lr']\n\n        # Compute full-image loss for comparison\n        full_loss = (mse.sum(dim=(1,2,3)) / (args.input_dim * args.train_img_height * args.train_img_height)).mean()\n\n        pbar.set_postfix({\n            'loss_query': f'{loss.item():.4f}',\n            'loss_full': f'{full_loss.item():.4f}',\n            'lr': f'{lr:.6f}'\n        })\n        writer.add_scalar('train/loss_query', loss.item(), count)\n        writer.add_scalar('train/loss_full', full_loss.item(), count)\n        writer.add_scalar('train/lr', lr, count)\n        start_time = time.time()\n\n    # Visualization\n    if count % args.vis_every == 0:\n        num_vis = min(args.vis_batch_size, 16)\n\n        vis_samples = []\n        for i in range(num_vis):\n            sample = sparse_dataset[i]\n            vis_samples.append({\n                'original': sample['image'],\n                'context': create_sparse_mask_image(\n                    sample['image'], sample['context_indices'], fill_value=0.5\n                ),\n                'query': create_sparse_mask_image(\n                    sample['image'], sample['query_indices'], fill_value=0.5\n                )\n            })\n\n        contexts_vis = torch.stack([s['context'] for s in vis_samples])\n        queries_vis = torch.stack([s['query'] for s in vis_samples])\n        originals_vis = torch.stack([s['original'] for s in vis_samples])\n\n        # Save comparison: [context | query | original]\n        fig_path = os.path.join(args.exp_path, 'samples', f'iter_{count:06d}.png')\n        comparison = torch.cat([contexts_vis, queries_vis, originals_vis], dim=0)\n        torchvision.utils.save_image(\n            comparison, fig_path, nrow=4, padding=2, normalize=True, value_range=(0, 1)\n        )\n\n        print(f'\\n[Iter {count}] Saved visualization to {fig_path}')\n        print(f'  Loss on query pixels (10%): {loss.item():.6f}')\n        print(f'  Loss on full image (100%): {full_loss.item():.6f}')\n\n    # Save checkpoint\n    if count % args.save_every == 0:\n        save_checkpoint(\n            args, count, loss.item(), gen_sde, optimizer, None, 'checkpoint.pt'\n        )\n        print(f'\\n[Iter {count}] Saved checkpoint')\n\npbar.close()\nprint('\\n' + '='*60)\nprint('Training completed!')\nprint('='*60)\nprint(f'Final query loss: {loss.item():.6f}')\nprint(f'Model trained to predict full field from 10% context,')\nprint(f'supervised only on 10% query pixels.')\nprint('='*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show latest visualization\n",
    "from PIL import Image\n",
    "\n",
    "latest_img = os.path.join(args.exp_path, 'samples', f'iter_{count:06d}.png')\n",
    "if os.path.exists(latest_img):\n",
    "    img = Image.open(latest_img)\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    plt.imshow(img)\n",
    "    plt.title(f'Training Progress at Iteration {count}\\n'\n",
    "              f'Top row: Context (10% input) | Middle row: Query (10% GT) | Bottom row: Original',\n",
    "              fontsize=12)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\nelse:\n",
    "    print(\"No visualization found yet\")\n",
    "\n",
    "print(f\"\\nExperiment path: {args.exp_path}\")\n",
    "print(f\"Samples: {os.path.join(args.exp_path, 'samples')}\")\n",
    "print(f\"TensorBoard: tensorboard --logdir={args.exp_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conditional Sampling (TODO)\n",
    "\n",
    "After training, we can generate reconstructions:\n",
    "1. Take an image from test set\n",
    "2. Extract context (10% pixels)\n",
    "3. Run reverse diffusion conditioned on context\n",
    "4. Compare reconstruction with ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement conditional sampling\n",
    "# This requires modifying the diffuse() method to accept context_image\n",
    "print(\"Conditional sampling implementation coming soon!\")\n",
    "print(\"\\nFor now, the model is training to denoise images conditioned on sparse context.\")\n",
    "print(\"The key achievement is that context information is now properly flowing through:\")\n",
    "print(\"  1. Fixed masks per image ✓\")\n",
    "print(\"  2. Context image created ✓\")\n",
    "print(\"  3. Context passed to model ✓\")\n",
    "print(\"  4. Model conditioned on context ✓\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}