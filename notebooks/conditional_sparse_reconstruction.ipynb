{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Sparse Image Reconstruction with DDO\n",
    "\n",
    "## Task\n",
    "Train a **conditional diffusion model** for sparse image reconstruction:\n",
    "- Each image has a **fixed** sparse mask (doesn't change during training)\n",
    "- 20% total allowance: **10% context** (input) + **10% query** (ground truth)\n",
    "- Model learns: `context (10%) → full image`, trained on query pixels\n",
    "\n",
    "## Key Features\n",
    "1. **Fixed masks per instance** - Each of 60K images has same mask every epoch\n",
    "2. **Conditional generation** - Model sees context as input\n",
    "3. **Query-based loss** - Loss computed only on 10% query pixels (or full image)\n",
    "4. **DDO framework** - Uses function-space diffusion with GP noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
    "display(HTML(\"<style>.output_result { max-width:98% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Change to parent directory\n",
    "os.chdir('..')\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utilities\n",
    "from utils.sparse_datasets_fixed import (\n",
    "    FixedSparseMaskDataset,\n",
    "    create_context_image_batched,\n",
    "    create_sparse_mask_image\n",
    ")\n",
    "from utils.visualize import get_grid_image\n",
    "from utils.utils import Writer, count_parameters_in_M, save_checkpoint, load_checkpoint\n",
    "from utils.ema import EMA\n",
    "\n",
    "# Import DDO components\n",
    "from lib.diffusion import BlurringDiffusion, DenoisingDiffusion\n",
    "from lib.models.fourier_unet import FNOUNet2d\n",
    "from lib.conditional_model import ConditionalDDOModel, ConditionalDDOModelSimple\n",
    "\n",
    "# Re-import tqdm after main imports\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "args = argparse.Namespace()\n",
    "\n",
    "# Paths\n",
    "args.exp_path = './experiments/conditional_sparse_recon'\n",
    "args.data = './data'\n",
    "args.seed = 1\n",
    "\n",
    "# Dataset\n",
    "args.dataset = 'cifar10'\n",
    "args.train_img_height = 32\n",
    "args.input_dim = 3\n",
    "args.coord_dim = 2\n",
    "\n",
    "# Sparse conditioning settings\n",
    "args.context_ratio = 0.1   # 10% for context (input)\n",
    "args.query_ratio = 0.1     # 10% for query (GT target)\n",
    "args.mask_seed = 42        # Seed for fixed masks\n",
    "\n",
    "# Model architecture\n",
    "args.model = 'fnounet2d'\n",
    "args.ch = 64                    # Base channels\n",
    "args.ch_mult = [1, 2, 2]        # Channel multipliers\n",
    "args.num_res_blocks = 2         # Residual blocks per level\n",
    "args.modes = 16                 # Fourier modes\n",
    "args.dropout = 0.1\n",
    "args.norm = 'group_norm'\n",
    "args.use_pos = True\n",
    "args.use_pointwise_op = True\n",
    "args.context_feature_dim = 32   # Context encoder output channels\n",
    "args.use_simple_conditioning = False  # True = simple concatenation, False = encoder\n",
    "\n",
    "# Diffusion settings (function-space DDO)\n",
    "args.ns_method = 'vp_cosine'\n",
    "args.timestep_sampler = 'low_discrepancy'\n",
    "args.disp_method = 'sine'\n",
    "args.sigma_blur_min = 0.05\n",
    "args.sigma_blur_max = 0.25\n",
    "args.gp_type = 'exponential'\n",
    "args.gp_exponent = 2.0\n",
    "args.gp_length_scale = 0.05\n",
    "args.gp_sigma = 1.0\n",
    "\n",
    "# Training\n",
    "args.train_batch_size = 128\n",
    "args.lr = 0.0002\n",
    "args.weight_decay = 0.0\n",
    "args.num_iterations = 100000\n",
    "args.ema_decay = 0.999\n",
    "args.optimizer = 'adam'\n",
    "args.beta1 = 0.9\n",
    "args.beta2 = 0.999\n",
    "\n",
    "# Logging\n",
    "args.print_every = 100\n",
    "args.save_every = 5000\n",
    "args.vis_every = 1000\n",
    "args.vis_batch_size = 16\n",
    "args.resume = True\n",
    "\n",
    "# Sampling\n",
    "args.num_steps = 250\n",
    "args.sampler = 'denoise'\n",
    "args.s_min = 0.0001\n",
    "\n",
    "# Misc\n",
    "args.distributed = False\n",
    "args.global_rank = 0\n",
    "args.checkpoint_file = 'checkpoint.pt'\n",
    "args.use_clip = False\n",
    "args.weight_method = None\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(args.exp_path, exist_ok=True)\n",
    "os.makedirs(os.path.join(args.exp_path, 'samples'), exist_ok=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Conditional Sparse Reconstruction Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Experiment: {args.exp_path}\")\n",
    "print(f\"Context ratio: {args.context_ratio*100:.0f}% (input)\")\n",
    "print(f\"Query ratio: {args.query_ratio*100:.0f}% (GT target)\")\n",
    "print(f\"Model: ch={args.ch}, ch_mult={args.ch_mult}, modes={args.modes}\")\n",
    "print(f\"Conditioning: {'Simple' if args.use_simple_conditioning else 'With Encoder'}\")\n",
    "print(f\"Iterations: {args.num_iterations}\")\n",
    "print(f\"Batch size: {args.train_batch_size}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset with Fixed Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "base_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=args.data, train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "print(f\"Base dataset: {len(base_dataset)} images\")\n",
    "\n",
    "# Wrap with fixed sparse masks\n",
    "sparse_dataset = FixedSparseMaskDataset(\n",
    "    dataset=base_dataset,\n",
    "    context_ratio=args.context_ratio,\n",
    "    query_ratio=args.query_ratio,\n",
    "    seed=args.mask_seed\n",
    ")\n",
    "\n",
    "print(sparse_dataset)\n",
    "\n",
    "# Create dataloader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    sparse_dataset,\n",
    "    batch_size=args.train_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDataloader: {len(train_loader)} batches per epoch\")\n",
    "print(f\"Total pixels: 32×32 = 1024\")\n",
    "print(f\"Context pixels: {sparse_dataset.num_context} (input)\")\n",
    "print(f\"Query pixels: {sparse_dataset.num_query} (GT target)\")\n",
    "print(f\"Remaining: {1024 - sparse_dataset.num_context - sparse_dataset.num_query} (not used)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Fixed Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples\n",
    "num_vis = 8\n",
    "originals = []\n",
    "contexts = []\n",
    "queries = []\n",
    "\n",
    "for i in range(num_vis):\n",
    "    sample = sparse_dataset[i]\n",
    "    originals.append(sample['image'])\n",
    "    \n",
    "    # Context visualization\n",
    "    contexts.append(create_sparse_mask_image(\n",
    "        sample['image'], sample['context_indices'], fill_value=0.5\n",
    "    ))\n",
    "    \n",
    "    # Query visualization\n",
    "    queries.append(create_sparse_mask_image(\n",
    "        sample['image'], sample['query_indices'], fill_value=0.5\n",
    "    ))\n",
    "\n",
    "originals = torch.stack(originals)\n",
    "contexts = torch.stack(contexts)\n",
    "queries = torch.stack(queries)\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "axes[0].imshow(get_grid_image(originals, nrow=4, to_numpy=True))\n",
    "axes[0].set_title('Original Images', fontsize=14)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(get_grid_image(contexts, nrow=4, to_numpy=True))\n",
    "axes[1].set_title('Context (10% Input) - FIXED per image', fontsize=14)\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(get_grid_image(queries, nrow=4, to_numpy=True))\n",
    "axes[2].set_title('Query (10% GT Target) - FIXED per image', fontsize=14)\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: Context and Query masks are FIXED for each image throughout training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Model with Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mgrid(dim, img_height):\n",
    "    \"\"\"Generate coordinate grid\"\"\"\n",
    "    grid = torch.linspace(0, img_height-1, img_height) / img_height\n",
    "    if dim == 2:\n",
    "        grid = torch.cat([grid[None,None,...,None].repeat(1, 1, 1, img_height),\n",
    "                          grid[None,None,None].repeat(1, 1, img_height, 1)], dim=1)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return grid\n",
    "\n",
    "\n",
    "def init_conditional_model(args):\n",
    "    \"\"\"Initialize conditional DDO model\"\"\"\n",
    "    \n",
    "    # GP config for function-space noise\n",
    "    gp_config = argparse.Namespace()\n",
    "    gp_config.device = 'cuda'\n",
    "    gp_config.exponent = args.gp_exponent\n",
    "    gp_config.length_scale = args.gp_length_scale\n",
    "    gp_config.sigma = args.gp_sigma\n",
    "    \n",
    "    # Blurring config\n",
    "    disp_config = argparse.Namespace()\n",
    "    disp_config.sigma_blur_min = args.sigma_blur_min\n",
    "    disp_config.sigma_blur_max = args.sigma_blur_max\n",
    "    \n",
    "    # Create diffusion process (function-space)\n",
    "    inf_sde = BlurringDiffusion(\n",
    "        dim=args.coord_dim,\n",
    "        ch=args.input_dim,\n",
    "        ns_method=args.ns_method,\n",
    "        disp_method=args.disp_method,\n",
    "        disp_config=disp_config,\n",
    "        gp_type=args.gp_type,\n",
    "        gp_config=gp_config,\n",
    "    )\n",
    "    \n",
    "    # Create base FNO-UNet\n",
    "    base_unet = FNOUNet2d(\n",
    "        modes_height=args.modes,\n",
    "        modes_width=args.modes,\n",
    "        in_channels=args.input_dim,\n",
    "        in_height=args.train_img_height,\n",
    "        ch=args.ch,\n",
    "        ch_mult=tuple(args.ch_mult),\n",
    "        num_res_blocks=args.num_res_blocks,\n",
    "        dropout=args.dropout,\n",
    "        norm=args.norm,\n",
    "        use_pos=args.use_pos,\n",
    "        use_pointwise_op=args.use_pointwise_op,\n",
    "    )\n",
    "    \n",
    "    # Wrap with conditional layer\n",
    "    if args.use_simple_conditioning:\n",
    "        print(\"Using simple conditioning (concatenation only)\")\n",
    "        model = ConditionalDDOModelSimple(base_unet, input_dim=args.input_dim)\n",
    "    else:\n",
    "        print(\"Using conditioning with encoder\")\n",
    "        model = ConditionalDDOModel(\n",
    "            base_unet,\n",
    "            input_dim=args.input_dim,\n",
    "            context_feature_dim=args.context_feature_dim\n",
    "        )\n",
    "    \n",
    "    # Create denoising diffusion wrapper\n",
    "    gen_sde = DenoisingDiffusion(\n",
    "        inf_sde,\n",
    "        model=model,\n",
    "        timestep_sampler=args.timestep_sampler,\n",
    "        use_clip=args.use_clip,\n",
    "        weight_method=args.weight_method\n",
    "    ).cuda()\n",
    "    \n",
    "    # Optimizer\n",
    "    if args.optimizer == \"adam\":\n",
    "        optimizer = torch.optim.Adam(\n",
    "            gen_sde.parameters(),\n",
    "            lr=args.lr,\n",
    "            betas=(args.beta1, args.beta2),\n",
    "            weight_decay=args.weight_decay\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {args.optimizer}\")\n",
    "    \n",
    "    # Add EMA\n",
    "    optimizer = EMA(optimizer, ema_decay=args.ema_decay)\n",
    "    \n",
    "    # Resume from checkpoint\n",
    "    count = 0\n",
    "    best_loss = 1e10\n",
    "    checkpoint_file = os.path.join(args.exp_path, args.checkpoint_file)\n",
    "    if args.resume and os.path.exists(checkpoint_file):\n",
    "        print(f'Loading checkpoint from {checkpoint_file}')\n",
    "        gen_sde, optimizer, _, count, best_loss = load_checkpoint(\n",
    "            checkpoint_file, gen_sde, optimizer, None\n",
    "        )\n",
    "        print(f'Resumed from iteration {count}')\n",
    "    \n",
    "    return gen_sde, optimizer, count, best_loss\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "gen_sde, optimizer, count, best_loss = init_conditional_model(args)\n",
    "\n",
    "# Count parameters\n",
    "num_params = count_parameters_in_M(gen_sde._model)\n",
    "print(f\"\\nModel parameters: {num_params:.2f}M\")\n",
    "print(f\"Starting from iteration: {count}\")\n",
    "print(f\"Best loss: {best_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conditional Training Loop\n",
    "\n",
    "Key differences from standard training:\n",
    "1. Fixed masks per image (same context every epoch)\n",
    "2. Context image passed to model during denoising\n",
    "3. Loss computed on full image (model learns full reconstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "writer = Writer(args.global_rank, args.exp_path)\n",
    "start_time = time.time()\n",
    "\n",
    "gen_sde.train()\n",
    "train_iter = iter(train_loader)\n",
    "pbar = tqdm(total=args.num_iterations, initial=count, desc='Conditional Training')\n",
    "\n",
    "# Pre-compute coordinate grid (same for all batches)\n",
    "v_grid = get_mgrid(2, args.train_img_height).cuda()\n",
    "\n",
    "while count < args.num_iterations:\n",
    "    try:\n",
    "        batch = next(train_iter)\n",
    "    except StopIteration:\n",
    "        train_iter = iter(train_loader)\n",
    "        batch = next(train_iter)\n",
    "    \n",
    "    # Get data\n",
    "    full_images = batch['image'].cuda()  # (B, C, H, W)\n",
    "    context_indices = batch['context_indices'].cuda()  # (B, num_context)\n",
    "    context_values = batch['context_values'].cuda()  # (B, num_context, C)\n",
    "    \n",
    "    batch_size = full_images.shape[0]\n",
    "    \n",
    "    # Create context image (sparse observations as dense image)\n",
    "    context_image = create_context_image_batched(\n",
    "        context_values,\n",
    "        context_indices,\n",
    "        height=args.train_img_height,\n",
    "        width=args.train_img_height,\n",
    "        num_channels=args.input_dim\n",
    "    )\n",
    "    \n",
    "    # Coordinate grid\n",
    "    v = v_grid.repeat(batch_size, 1, 1, 1)\n",
    "    \n",
    "    # Forward pass with conditioning\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # DSM loss with context conditioning\n",
    "    # The model will receive context_image through **kwargs\n",
    "    loss = gen_sde.dsm(full_images, v, context_image=context_image).mean()\n",
    "    \n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    count += 1\n",
    "    pbar.update(1)\n",
    "    \n",
    "    # Logging\n",
    "    if count % args.print_every == 0:\n",
    "        elapsed = (time.time() - start_time) / args.print_every\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'lr': f'{lr:.6f}',\n",
    "            's/it': f'{elapsed:.2f}'\n",
    "        })\n",
    "        writer.add_scalar('train/loss', loss.item(), count)\n",
    "        writer.add_scalar('train/lr', lr, count)\n",
    "        start_time = time.time()\n",
    "    \n",
    "    # Visualization\n",
    "    if count % args.vis_every == 0:\n",
    "        # Save visualization\n",
    "        num_vis = min(args.vis_batch_size, 16)\n",
    "        \n",
    "        vis_samples = []\n",
    "        for i in range(num_vis):\n",
    "            sample = sparse_dataset[i]\n",
    "            vis_samples.append({\n",
    "                'original': sample['image'],\n",
    "                'context': create_sparse_mask_image(\n",
    "                    sample['image'], sample['context_indices'], fill_value=0.5\n",
    "                ),\n",
    "                'query': create_sparse_mask_image(\n",
    "                    sample['image'], sample['query_indices'], fill_value=0.5\n",
    "                )\n",
    "            })\n",
    "        \n",
    "        contexts_vis = torch.stack([s['context'] for s in vis_samples])\n",
    "        queries_vis = torch.stack([s['query'] for s in vis_samples])\n",
    "        originals_vis = torch.stack([s['original'] for s in vis_samples])\n",
    "        \n",
    "        # Save comparison: [context | query | original]\n",
    "        fig_path = os.path.join(args.exp_path, 'samples', f'iter_{count:06d}.png')\n",
    "        comparison = torch.cat([contexts_vis, queries_vis, originals_vis], dim=0)\n",
    "        torchvision.utils.save_image(\n",
    "            comparison, fig_path, nrow=4, padding=2, normalize=True, value_range=(0, 1)\n",
    "        )\n",
    "        \n",
    "        print(f'\\n[Iter {count}] Saved visualization to {fig_path}')\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if count % args.save_every == 0:\n",
    "        save_checkpoint(\n",
    "            args, count, loss.item(), gen_sde, optimizer, None, 'checkpoint.pt'\n",
    "        )\n",
    "        print(f'\\n[Iter {count}] Saved checkpoint')\n",
    "\n",
    "pbar.close()\n",
    "print('\\n' + '='*60)\n",
    "print('Training completed!')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show latest visualization\n",
    "from PIL import Image\n",
    "\n",
    "latest_img = os.path.join(args.exp_path, 'samples', f'iter_{count:06d}.png')\n",
    "if os.path.exists(latest_img):\n",
    "    img = Image.open(latest_img)\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    plt.imshow(img)\n",
    "    plt.title(f'Training Progress at Iteration {count}\\n'\n",
    "              f'Top row: Context (10% input) | Middle row: Query (10% GT) | Bottom row: Original',\n",
    "              fontsize=12)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\nelse:\n",
    "    print(\"No visualization found yet\")\n",
    "\n",
    "print(f\"\\nExperiment path: {args.exp_path}\")\n",
    "print(f\"Samples: {os.path.join(args.exp_path, 'samples')}\")\n",
    "print(f\"TensorBoard: tensorboard --logdir={args.exp_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conditional Sampling (TODO)\n",
    "\n",
    "After training, we can generate reconstructions:\n",
    "1. Take an image from test set\n",
    "2. Extract context (10% pixels)\n",
    "3. Run reverse diffusion conditioned on context\n",
    "4. Compare reconstruction with ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement conditional sampling\n",
    "# This requires modifying the diffuse() method to accept context_image\n",
    "print(\"Conditional sampling implementation coming soon!\")\n",
    "print(\"\\nFor now, the model is training to denoise images conditioned on sparse context.\")\n",
    "print(\"The key achievement is that context information is now properly flowing through:\")\n",
    "print(\"  1. Fixed masks per image ✓\")\n",
    "print(\"  2. Context image created ✓\")\n",
    "print(\"  3. Context passed to model ✓\")\n",
    "print(\"  4. Model conditioned on context ✓\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
